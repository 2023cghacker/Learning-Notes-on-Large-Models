通过源码学习AI算法是高效、高收益的方式。以下是精选的**10大必看源码项目**，并按照**学习难度和推荐时间顺序排序**，每个源码都附带“**为什么要看**”、“**学什么**”以及“**注意事项**”。

---

## ✅ 难度 & 时间顺序建议：

| 编号 | 源码名称                                                                      | 推荐阶段   | 难度    | 推荐阅读时间 |
| -- | ------------------------------------------------------------------------- | ------ | ----- | ------ |
| ①  | [Transformers (Huggingface)](https://github.com/huggingface/transformers) | 入门/打基础 | ★★★   | 1-2周   |
| ②  | [LLaMA (Meta)](https://github.com/facebookresearch/llama)                 | 模型理解   | ★★★★  | 1-2周   |
| ③  | [GPT-2 / GPT-3](https://github.com/openai/gpt-2)（OpenAI官方或第三方复现）          | 中级/重点  | ★★★★  | 1周     |
| ④  | [minGPT](https://github.com/karpathy/minGPT)                              | 快速掌握原理 | ★     | 1-2天   |
| ⑤  | [nanoGPT](https://github.com/karpathy/nanoGPT)                            | 代码实战   | ★★    | 3-4天   |
| ⑥  | [LoRA / PEFT](https://github.com/huggingface/peft)                        | 精调进阶   | ★★★   | 3天     |
| ⑦  | [FlashAttention](https://github.com/HazyResearch/flash-attention)         | 模型优化   | ★★★★★ | 1周     |
| ⑧  | [DeepSpeed](https://github.com/microsoft/DeepSpeed)                       | 系统加速   | ★★★★★ | 1-2周   |
| ⑨  | [vLLM](https://github.com/vllm-project/vllm)                              | 推理引擎   | ★★★★  | 1周     |
| ⑩  | [OpenChat / ChatGLM / InternLM](https://github.com/THUDM/ChatGLM2-6B)     | 国内大模型  | ★★★   | 3-5天   |

---

## 🔍 逐个解读与推荐理由

---

### ① **Transformers（Huggingface）**

* **学什么：** BERT/GPT/T5等架构的标准化封装，Trainer、Tokenizer、Model等模块。
* **为什么：** 面试必问的基础，代码规范、模块划分清晰，非常适合打基础。
* **注意：** 推荐从 `modeling_bert.py` / `modeling_gpt2.py` 开始看。

---

### ② **LLaMA（Meta）**

* **学什么：** 高性能大模型的训练细节，位置编码、RMSNorm、预训练配置。
* **为什么：** 国内外很多模型（如ChatGLM）基于LLaMA架构，掌握它是理解其他模型的关键。
* **注意：** 关注 `model.py` 中的`TransformerBlock`、`RMSNorm`、位置编码处理。

---

### ③ **GPT-2 / GPT-3**

* **学什么：** GPT系列的原始结构，残差连接、层归一化、因果注意力。
* **为什么：** GPT系列是所有大模型的核心基础，必须掌握。
* **注意：** GPT-3源码不开源，推荐看[`rwkv`](https://github.com/BlinkDL/RWKV-LM)或 [`gpt-neox`](https://github.com/EleutherAI/gpt-neox) 这类复现。

---

### ④ **minGPT（Karpathy）**

* **学什么：** 最简洁的GPT实现，帮助理解attention、训练流程。
* **为什么：** 可快速理解transformer最核心逻辑，几百行代码。
* **注意：** 适合搭配阅读论文（如Attention is All You Need）后食用。

---

### ⑤ **nanoGPT**

* **学什么：** 从数据预处理到训练、推理，完整小模型训练pipeline。
* **为什么：** 实战派首选，适合自己动手改造和实验。
* **注意：** 支持wandb记录、分布式训练，结构与大模型训练流程一致。

---

### ⑥ **LoRA / PEFT**

* **学什么：** 轻量级微调方法，如Low-Rank Adapter。
* **为什么：** 面试常问“如何在算力不足的情况下微调大模型”，掌握LoRA是必备。
* **注意：** 推荐理解 `LoraLayer` 和 `get_peft_model` 逻辑。

---

### ⑦ **FlashAttention**

* **学什么：** 高效attention实现，用于加速大模型训练。
* **为什么：** 几乎所有大模型都集成此优化，了解CUDA kernel层的加速机制也能拉开面试差距。
* **注意：** 可选深度探索或跳读（先看论文+repo结构）。

---

### ⑧ **DeepSpeed**

* **学什么：** ZeRO优化、pipeline并行、Offload、大规模训练。
* **为什么：** 训练大模型绕不开DeepSpeed，许多企业也使用。
* **注意：** 难度较高，可重点关注 ZeRO-1/2/3 的memory优化逻辑。

---

### ⑨ **vLLM**

* **学什么：** 高性能推理引擎、KV缓存管理、连续批处理。
* **为什么：** 推理效率优化是面试加分点，vLLM是开源代表。
* **注意：** 理解 token streaming、KV cache 的重用策略。

---

### ⑩ **OpenChat / ChatGLM / InternLM**

* **学什么：** 国内自研大模型的结构、训练方式、推理接口。
* **为什么：** 面试中常被问“国内模型与国外的差异”，熟悉这些项目是竞争优势。
* **注意：** ChatGLM基于GLM架构，区别于GPT，需注意结构上的不同。

---

## 🎯 延伸补充建议

* **阅读顺序建议**：

  1. 先看 minGPT → nanoGPT → GPT2 → LLaMA
  2. 再看 Transformers → LoRA/PEFT → FlashAttention
  3. 最后攻坚 DeepSpeed / vLLM / 国内大模型
* **阅读方式建议**：

  * 跟论文对照阅读（如LLaMA论文 vs LLaMA源码）
  * 实践为主，边改边学（如nanoGPT可替换数据训练中文模型）

---
